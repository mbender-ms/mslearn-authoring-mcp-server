---
title: Appendix 2 Human oversight - Responsible AI Style Guide
description: Explore the limitations of human oversight in addressing fairness issues in AI systems. Understand cognitive biases like confirmation and automation bias that impact decision-making. Learn more about these challenges and their implications for AI fairness.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60815
---


# Appendix 2: Human oversight

Human oversight is not enough for addressing fairness issues in the context of AI systems.  
There are various *cognitive biases* to consider that make relying on human oversight to address fairness issues insufficient:

- One consideration is *confirmation bias;* when human decision-makers get outputs from AI systems that don’t match their decisions, they often override the systems’ outputs. For example, [recidivism algorithms](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) intended to aid prison sentencing can be overridden by human judges but may still exhibit [disparate outcomes](https://www.law.harvard.edu/programs/olin_center/Prizes/2019-1.pdf).  

- Another consideration is *automation bias,* meaning people over-relying on AI systems’ recommendations without scrutinizing them. This can lead to the amplification of existing unfairness in AI systems.

See the [Overreliance on AI literature review](https://www.microsoft.com/research/publication/overreliance-on-ai-literature-review/) for more about human oversight.  
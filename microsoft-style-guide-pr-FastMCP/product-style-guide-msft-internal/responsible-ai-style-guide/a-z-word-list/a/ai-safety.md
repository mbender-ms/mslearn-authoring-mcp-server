---
title: AI safety - Responsible AI Style Guide
description: Learn how to refer to "AI safety" in your content.
ms.date: 11/12/2024
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 71645
---


# AI safety

This term is increasingly used instead of [*responsible AI*](~\responsible-ai-style-guide\a-z-word-list\r\responsible-ai.md) (for example, the NIST initiative, [U.S. AI Safety Institute](https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute)). It is also sometimes used as shorthand for *AI reliability and safety,* one of the six [Microsoft AI principles](~\responsible-ai-style-guide\a-z-word-list\a\ai-principles.md). 

However, it is important to **be aware that there are two fundamentally different contexts** in which the term *AI safety* is used:

1. [Responsible AI](~\responsible-ai-style-guide\a-z-word-list\r\responsible-ai.md)—which people sometimes mean when using *AI safety*—is the goal of the six Microsoft [AI principles](~\responsible-ai-style-guide\a-z-word-list\a\ai-principles.md) that include fairness; reliability and safety; privacy and security; inclusiveness; accountability; and transparency. Responsible AI is a human-centered approach to creating AI systems that benefit people while mitigating harms, and is achieved through research-driven best practices. [Human-centered AI](~\responsible-ai-style-guide\a-z-word-list\h\human-centered-ai.md) is about ensuring that *what* we build benefits people and society, and that *how* we build it begins and ends with people in mind—[creating AI to solve real problems with responsible solutions](https://www.microsoft.com/research/blog/advancing-human-centered-ai-updates-on-responsible-ai-research/).

In addition, the Microsoft AI principle of reliability and safety—which people sometimes conversationally shorten to *AI safety*—means, in brief, that an AI system should be designed in such a way that it works as intended for its supported uses (the primary purposes for which people are expected to use the system), under diverse conditions, over its lifetime, and when under attack by adversaries ([source](https://microsoft.sharepoint.com/:p:/s/Aether/ESmvn3lZbL9FmdhKj9lOT-sBJozTLMhhxE7glP48al_AsQ?e=deejla)).

See [Top tip: Structure discussion of AI around the six Microsoft AI principles](~\responsible-ai-style-guide\top-tips\structure-discussion-of-ai-around-the-six-microsoft-ai-principles.md).

2. In contrast, ideologies of longtermism, transhumanism, and others* use the term *AI safety* to refer to efforts for thwarting a hypothetical existential risk to humanity in discussions ([often profiled in the media](https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html)) of [artificial general intelligence (AGI)](~\responsible-ai-style-guide\a-z-word-list\a\artificial-general-intelligence-agi.md) and superintelligence—defined as "an intellect that is much smarter than the best human brains in practically every field" ([source](https://nickbostrom.com/superintelligence)).

For a variety of reasons, including an emphasis on conjecture about the far future ([source](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf)), these ideologies and their usage of *AI safety* do not align with Microsoft values as expressed in its [mission](https://www.microsoft.com/about#:~:text=Our%20mission%20is%20to%20empower,the%20planet%20to%20achieve%20more.) to empower every person and every organization on the planet to achieve more.

*For more about these ideologies and their communities, see:

- [Eugenics and the Promise of Utopia Through Artificial General Intelligence](https://www.youtube.com/watch?v=P7XT4TWLzJw)
- [The Dangerous Ideas of "Longtermism" and "Existential Risk"](https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk)
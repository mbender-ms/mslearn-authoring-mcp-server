---
title: red team, red teaming - Responsible AI Style Guide
description: Learn how to refer to "red team, red teaming" in your content.
ms.date: 07/19/2023
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 71653
---


# red team, red teaming

The term *red teaming* has historically described systematic adversarial attacks for testing software security vulnerabilities. With the rise of AI and, specifically, large language models (LLMs), the term has extended beyond traditional cybersecurity and evolved in common usage to describe many kinds of probing, testing, and attacking of AI systems for the purpose of uncovering and identifying potentially harmful outputs that can take many forms, including hate speech, incitement or glorification of violence, sexual content, or misinformation.

*Red teaming* is an essential practice in the responsible development of systems and features using LLMs because the uncovering and identifying of harms enables systematic measurement and mitigation of such harms.
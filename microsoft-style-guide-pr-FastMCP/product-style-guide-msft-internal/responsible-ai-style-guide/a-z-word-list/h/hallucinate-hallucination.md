---
title: hallucinate, hallucination - Responsible AI Style Guide
description: Learn how to refer to "hallucinate, hallucination" in your content.
ms.date: 11/13/2024
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 63213
---


# hallucinate, hallucination

A term originating in the AI research community that refers to the phenomenon of large language models (LLMs) sometimes generating responses that are factually incorrect or incoherent. The term *hallucinate* is not recommended for use in communication about LLMs. Attributing *hallucination* to LLMs anthropomorphizes them (see [Donâ€™t humanize AI](~\responsible-ai-style-guide\top-tips\dont-humanize-ai.md)) and can also be offensive to people affected by illnesses that cause hallucinations. Moreover, hallucination is a pathological symptom in people, whereas it is part of normal operation in LLMs. Instead, consider using language such as *LLMs sometimes* fabricate *information.* (Note, *fabricate* is a term commonly used to describe manufacturing processes.)

However, for precision and clarity when communicating about a distinct issue with the text generated by LLMs, avoid using catchall terms. Be specific and [use easy-to-understand language](~\responsible-ai-style-guide\top-tips\use-easy-to-understand-language-to-describe-ai-systems.md). For example, use descriptive phrases like *LLMs can generate text that is factually incorrect*; *LLMs can generate text that is incoherent*; or *LLMs can generate text that misrepresents or does not exist in a given information source.*
---
title: Be specific when talking about fairness - Responsible AI Style Guide
description: Learn how to effectively discuss fairness in AI by specifying deployment contexts, potential harms, and affected groups. Avoid vague statements to ensure clarity and accuracy in your content.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60771
---


# Be specific when talking about fairness

Avoid talking about fairness in AI in general ways. Instead, specify the deployment context or [use of AI](~\responsible-ai-style-guide\uses-ai-terms\uses-of-aiterms-for-categories-of-ai-use.md), the particular [fairness-related harms](~\responsible-ai-style-guide\fairness\related-harms\fairness-related-harms.md), and the groups of people that may be affected. 

(See [Don’t overclaim or misrepresent what an AI system can do.](~\responsible-ai-style-guide\top-tips\dont-overclaim-or-misrepresent-what-ai-can-do.md)) 

| Use this | Not this |
|----------|----------|
| While this system works well for some groups, it’s known that women with darker skin color experience underrepresentation in the training data and quality-of-service harms when interacting with facial recognition systems. | Some groups may face issues using facial recognition systems. |
---
title: Communicate that AI systems can behave unfairly for a variety of reasons - Responsible AI Style Guide
description: Explore the complexities of AI fairness and understand why there is no universal solution to prevent unfair behavior in AI systems. Learn how development teams can identify, measure, and mitigate fairness-related harms effectively.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60769
---


# Communicate that AI systems can behave unfairly for a variety of reasons

AI systems can behave unfairly for a variety of reasons, some social, some technical, and some a combination of both social and technical. These include, but are not limited to:

- Societal biases reflected in training datasets. 
- Societal biases that are either explicitly or implicitly reflected in decisions made by teams during the AI development and deployment life cycle. 
- AI system characteristics that, while not necessarily reflective of societal biases, can result in unfair behavior when these systems interact with some stakeholders after deployment. 

Because of the many reasons that may cause an AI system to behave unfairly, there is no single, one-size-fits-all solution. **Teams tasked with developing and deploying AI systems must instead work to identify, measure, and mitigate fairness-related harms as much as possible.**  
---
title: Talking about mitigating fairness-related harms - Responsible AI Style Guide
description: Explore strategies for mitigating fairness-related harms in AI systems. Understand the limitations of "de-biasing" and the importance of identifying, measuring, and addressing fairness issues.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60777
---


# Talking about mitigating fairness-related harms

_Set appropriate expectations when talking about mitigation strategies._ As noted throughout this style guide, it’s important to avoid language about “de-biasing” AI systems or “guaranteeing” fairness because it is not possible to fully “de-bias” an AI system. Instead, emphasize that teams tasked with developing and deploying AI systems should work to identify, measure, and mitigate fairness-related harms as much as possible.

**Be clear that:**

- **There are no perfect solutions for fairness in AI.**
- **No single measurement alone can address fairness harms.**
- **Human oversight doesn’t resolve fairness issues.**

Note: Human-oversight strategies can be a meaningful point of accountability. But [human oversight](~\responsible-ai-style-guide\a-z-word-list\h\human-oversight.md) is not a sufficient or always effective means for addressing AI fairness issues and is not a strategy for reducing development teams’ responsibility. (See [Appendix 2](~\responsible-ai-style-guide\appendix\appendix-2-human-oversight-.md) for more about how human oversight is not enough for addressing fairness issues.)

Learn more about resources for mitigating fairness-related harms [here](https://microsoft.sharepoint.com/teams/Aether/SitePages/Fairness-and-Inclusiveness.aspx).
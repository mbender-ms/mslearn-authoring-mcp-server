---
title: Talking about measuring fairness-related harms - Responsible AI Style Guide
description: Explore the complexities of measuring fairness-related harms in AI systems. Understand why no single metric can fully address these issues and the importance of using multiple metrics to evaluate AI performance. Learn more about the limitations of quantifiable metrics and the societal factors they may overlook.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60776
---


# Talking about measuring fairness-related harms

Communicate that **no single measurement alone can address fairness-related harms.** Quantifiable metrics, such as parity metrics used for determining fairness, are only evaluations of technical systems and donâ€™t measure or reflect larger structural forces within society. For example, systemic injustices often exist in the social contexts where AI systems are used and are not detected by parity metrics.

**Be clear that:**

- No single measurement alone can address fairness-related harms. 
- Any quantitative definition of fairness will omit aspects of societal concepts of fairness that cannot be quantified, such as justice, due process, and so on. 
- Multiple metrics should be used to show that AI system performance can vary. 
- Many assumptions are built into metrics and benchmark datasets. 
- Fairness metrics are **not** complete or comprehensive.

Learn more about resources for identifying and measuring fairness-related harms [here](https://microsoft.sharepoint.com/teams/Aether/SitePages/Fairness-and-Inclusiveness.aspx).
---
title: Intelligibility - Responsible AI Style Guide
description: Learn how to ensure AI systems are understandable and transparent with our guide on intelligibility. Discover how to design and document AI systems for clear user interaction and effective mental modeling.
ms.date: 12/13/2022
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 60812
---


# Intelligibility

*Intelligibility* means that people should be able to understand, monitor, and respond to the behavior of AI systems. In other words, to achieve intelligibility an AI system should be designed and documented in a way that enables people to build a coherent mental model of how the system works, including through their interaction with the system. This applies not only to an AI system’s end users, but also the system’s engineers, model developers, and, potentially, the people who are affected by the system.

***Intelligibility* is a comprehensive term that accounts for the [sociotechnical](~\responsible-ai-style-guide\a-z-word-list\s\sociotechnical.md) requirements for transparency in AI systems.** Intelligibility is impacted by model components, user interfaces, and documentation (e.g., Transparency Notes), which help people understand the characteristics and limitations of an AI system.

**Note:** You may sometimes see industry teams and researchers using the terms [*interpretability*](~\responsible-ai-style-guide\a-z-word-list\i\interpretability.md) or [*explainability*](~\responsible-ai-style-guide\a-z-word-list\e\explainability.md) to refer to algorithmic techniques for generating visibility into system or model behavior for technical insight—and sometimes these terms are used interchangeably with *intelligibility.* However, for an AI system to achieve *intelligibility,* more than technical insight to model behavior is often required; for example, people may need to have appropriate explanations or interaction with an AI system to be able to form an understanding of the system’s behavior. 
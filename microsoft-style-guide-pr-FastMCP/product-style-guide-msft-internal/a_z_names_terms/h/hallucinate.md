---
title: hallucinate
description: Learn how to refer to "hallucinate" in your content.
ms.date: 03/22/2024
ms.topic: contributor-guide
ms.service: microsoft-product-style-guide
ms.custom:
  - TopicID 63310
---


# hallucinate

Do not use.  

Hallucinate refers to fabricated responses that sound plausible but are inaccurate. These responses often emerge from the AI model's inherent biases, lack of real-world understanding, or training data limitations. In other words, an AI system might "hallucinate" or fabricate to fill in for gaps it has not been explicitly trained on, leading to inaccurate outputs.

**Guidelines**  

Do not use hallucinate/hallucination (supported by CELA and [RAI guidance](~\responsible-ai-style-guide\a-z-word-list\h\hallucinate-hallucination.md)). 
 
Instead, use words or phrases like:

- inaccurate
- might contain inaccuracies
- might be inaccurate

**For example:**  

- prompt responses might contain inaccuracies.
- responses to question prompts might contain inaccurate information.
- “… might produce inaccurate information about people, places, or facts” (source: ChatGPT disclaimer).

Other terms to avoid:  

- misinformation
- fabrication
- misleading
- biased
- false

How hallucinations can be messaged instead? Examples:

- ChatGPT might produce inaccurate information about people, places, or facts.
- Might occasionally generate inaccurate information.
- Let’s learn together. Bing is powered by AI, so surprises and mistakes are possible. Make sure to check the facts, and share feedback so we can learn and improve.

**See also** [inaccurate](https://styleguides.azurewebsites.net/Styleguide/Read?id=2696&topicid=63313)

